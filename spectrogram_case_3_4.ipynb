{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "U9ksN6VYbDhR",
    "outputId": "be431b7b-27a1-4694-d8cc-5893f65f7982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from shutil import copyfile\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "kf = KFold(n_splits=5, shuffle= True, random_state=28)\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from shutil import copyfile\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "kf = KFold(n_splits=5, shuffle= True, random_state=28)\n",
    "from scipy import stats\n",
    "import python_speech_features\n",
    "from essentia.standard import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tải dữ liệu từ Google Drive lên Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBBHIibM6Aor"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/My\\ Drive/DuLieu/filter_feature/s.npy .\n",
    "!cp /content/drive/My\\ Drive/DuLieu/filter_feature/s_l.npy .\n",
    "!cp /content/drive/My\\ Drive/DuLieu/filter_feature/s_a.npy .\n",
    "!cp /content/drive/My\\ Drive/DuLieu/filter_feature/s_a_l.npy ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8YAHrIT5ZCd"
   },
   "outputs": [],
   "source": [
    "class LogConfusionMatrix(keras.callbacks.Callback):\n",
    "  def __init__(self, names):\n",
    "    self.names = names\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.bestacc = 0.0\n",
    "    self.bestcf = None\n",
    "  def on_epoch_end(self, epoch, logs = {}):\n",
    "    if logs[\"val_acc\"] > self.bestacc:\n",
    "      self.bestacc = logs[\"val_acc\"] \n",
    "      y_pred = self.model.predict(self.validation_data[0])\n",
    "      y_pred = [1 * (x[0]>=0.5) for x in y_pred]\n",
    "      self.bestcf = confusion_matrix(self.validation_data[1], y_pred)\n",
    "  def on_train_end(self, logs = {}):\n",
    "    f = open(self.names, \"a\")\n",
    "    f.write(\"Best val_acc: \" + str(self.bestacc))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Best cf: \\n\" + str(self.bestcf))\n",
    "    f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_9hrFlOjgoS"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(64, (6, 6), activation = \"relu\", padding = \"valid\", input_shape = (513, 94, 1)))\n",
    "  model.add(MaxPooling2D(2, 2))\n",
    "  model.add(Conv2D(64, (6, 6), activation = \"relu\", padding = \"valid\"))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(400))\n",
    "  model.add(Activation(\"relu\"))\n",
    "  model.add(Dense(150))\n",
    "  model.add(Activation(\"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(1, activation = \"sigmoid\"))\n",
    "  model.compile(optimizer= keras.optimizers.Adam(lr=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciXDjJmo7TEF"
   },
   "outputs": [],
   "source": [
    "def train_case_3(X_train, y_train, X_test, y_test,filename, epochs = 50):\n",
    "  model = create_model()\n",
    "  logcf = LogConfusionMatrix(filename)\n",
    "  es = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0.008, patience=9, verbose=1, mode='auto')\n",
    "  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = \"loss\",patience = 3, factor = 0.8, min_lr = 0.0001, verbose = 1)\n",
    "  model.fit(X_train, y_train, epochs = epochs, batch_size = 64, validation_data = (X_test, y_test), callbacks = [es, logcf, reduce_lr])\n",
    "  for j in range(30):\n",
    "    gc.collect()\n",
    "  time.sleep(5)\n",
    "  for j in range(30):\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 802
    },
    "colab_type": "code",
    "id": "Br6A2ucU0jJe",
    "outputId": "45fdd42f-aa4a-4666-b44c-a8c63272952e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update version\n",
      "Train on 38710 samples, validate on 5492 samples\n",
      "Epoch 1/120\n",
      "38710/38710 [==============================] - 71s 2ms/step - loss: 0.2279 - acc: 0.9074 - val_loss: 0.3119 - val_acc: 0.8786\n",
      "Epoch 2/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.1058 - acc: 0.9593 - val_loss: 0.4611 - val_acc: 0.8379\n",
      "Epoch 3/120\n",
      "38710/38710 [==============================] - 70s 2ms/step - loss: 0.0798 - acc: 0.9698 - val_loss: 0.3547 - val_acc: 0.8578\n",
      "Epoch 4/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0611 - acc: 0.9771 - val_loss: 0.2680 - val_acc: 0.9064\n",
      "Epoch 5/120\n",
      "38710/38710 [==============================] - 70s 2ms/step - loss: 0.0494 - acc: 0.9822 - val_loss: 0.2395 - val_acc: 0.9151\n",
      "Epoch 6/120\n",
      "38710/38710 [==============================] - 70s 2ms/step - loss: 0.0385 - acc: 0.9856 - val_loss: 0.2071 - val_acc: 0.9350\n",
      "Epoch 7/120\n",
      "38710/38710 [==============================] - 70s 2ms/step - loss: 0.0361 - acc: 0.9877 - val_loss: 0.2030 - val_acc: 0.9345\n",
      "Epoch 8/120\n",
      "38710/38710 [==============================] - 70s 2ms/step - loss: 0.0304 - acc: 0.9890 - val_loss: 0.2187 - val_acc: 0.9295\n",
      "Epoch 9/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0289 - acc: 0.9898 - val_loss: 0.2242 - val_acc: 0.9223\n",
      "Epoch 10/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0234 - acc: 0.9918 - val_loss: 0.2026 - val_acc: 0.9426\n",
      "Epoch 11/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0203 - acc: 0.9931 - val_loss: 0.1915 - val_acc: 0.9463\n",
      "Epoch 12/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0186 - acc: 0.9935 - val_loss: 0.2183 - val_acc: 0.9330\n",
      "Epoch 13/120\n",
      "38710/38710 [==============================] - 70s 2ms/step - loss: 0.0148 - acc: 0.9951 - val_loss: 0.1991 - val_acc: 0.9437\n",
      "Epoch 14/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0181 - acc: 0.9935 - val_loss: 0.2197 - val_acc: 0.9405\n",
      "Epoch 15/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0105 - acc: 0.9967 - val_loss: 0.2324 - val_acc: 0.9377\n",
      "Epoch 16/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.3213 - val_acc: 0.9261\n",
      "Epoch 17/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0115 - acc: 0.9964 - val_loss: 0.2095 - val_acc: 0.9436\n",
      "Epoch 18/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0107 - acc: 0.9964 - val_loss: 0.1725 - val_acc: 0.9539\n",
      "Epoch 19/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0098 - acc: 0.9973 - val_loss: 0.2855 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "Epoch 20/120\n",
      "38710/38710 [==============================] - 69s 2ms/step - loss: 0.0116 - acc: 0.9966 - val_loss: 0.1937 - val_acc: 0.9439\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(\"s_a.npy\")\n",
    "y_train = np.load(\"s_a_l.npy\")\n",
    "X_test = np.load(\"s.npy\")\n",
    "y_test = np.load(\"s_l.npy\")\n",
    "print(\"update version\")\n",
    "t = X_train.shape\n",
    "X_train = X_train.reshape(t[0], t[1], t[2], 1)\n",
    "t = X_test.shape\n",
    "X_test = X_test.reshape(t[0], t[1], t[2], 1)\n",
    "for j in range(30):\n",
    "  gc.collect()\n",
    "time.sleep(5)\n",
    "for j in range(30):\n",
    "  gc.collect()\n",
    "mean = np.mean(X_train, axis = 0)\n",
    "std = np.std(X_train, axis = 0)\n",
    "X_train = stats.zscore(X_train)\n",
    "X_test = (X_test - mean)/std\n",
    "train_case_3(X_train, y_train, X_test, y_test, filename = \"spectrogram_case_3.txt\", epochs = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGfgdvtLl8Yx"
   },
   "outputs": [],
   "source": [
    "def train_case_4(X_train, y_train, X_test, y_test,filename, epochs = 50):\n",
    "  model = create_model()\n",
    "  logcf = LogConfusionMatrix(filename)\n",
    "  es = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0.001, patience=15, verbose=1, mode='auto')\n",
    "  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = \"loss\",patience = 5, factor = 0.9, min_lr = 0.0001, verbose = 1)\n",
    "  model.fit(X_train, y_train, epochs = epochs, batch_size = 64, validation_data = (X_test, y_test), callbacks = [es, logcf, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DcjtzzgO0U6x",
    "outputId": "9a5a771b-2b19-47db-a538-d659d8359e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 5492 samples, validate on 38710 samples\n",
      "Epoch 1/120\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "5492/5492 [==============================] - 37s 7ms/step - loss: 0.5551 - acc: 0.7775 - val_loss: 0.5874 - val_acc: 0.7189\n",
      "Epoch 2/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.4504 - acc: 0.8063 - val_loss: 0.6397 - val_acc: 0.7045\n",
      "Epoch 3/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.4487 - acc: 0.8105 - val_loss: 0.5711 - val_acc: 0.7219\n",
      "Epoch 4/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.4214 - acc: 0.8188 - val_loss: 0.6744 - val_acc: 0.7156\n",
      "Epoch 5/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.3982 - acc: 0.8398 - val_loss: 0.4968 - val_acc: 0.7727\n",
      "Epoch 6/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.3371 - acc: 0.8631 - val_loss: 0.5524 - val_acc: 0.7715\n",
      "Epoch 7/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.2852 - acc: 0.8946 - val_loss: 0.4825 - val_acc: 0.8059\n",
      "Epoch 8/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.2272 - acc: 0.9142 - val_loss: 0.6176 - val_acc: 0.8013\n",
      "Epoch 9/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.1909 - acc: 0.9303 - val_loss: 0.5804 - val_acc: 0.8225\n",
      "Epoch 10/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.1709 - acc: 0.9390 - val_loss: 0.5715 - val_acc: 0.8311\n",
      "Epoch 11/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.1456 - acc: 0.9457 - val_loss: 0.7367 - val_acc: 0.8234\n",
      "Epoch 12/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.1114 - acc: 0.9630 - val_loss: 0.9413 - val_acc: 0.8151\n",
      "Epoch 13/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.1093 - acc: 0.9610 - val_loss: 0.9490 - val_acc: 0.8236\n",
      "Epoch 14/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.0821 - acc: 0.9723 - val_loss: 0.6907 - val_acc: 0.8629\n",
      "Epoch 15/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0753 - acc: 0.9716 - val_loss: 1.4498 - val_acc: 0.7896\n",
      "Epoch 16/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0520 - acc: 0.9814 - val_loss: 0.9874 - val_acc: 0.8470\n",
      "Epoch 17/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.0524 - acc: 0.9823 - val_loss: 1.1537 - val_acc: 0.8298\n",
      "Epoch 18/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0430 - acc: 0.9860 - val_loss: 0.9550 - val_acc: 0.8568\n",
      "Epoch 19/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0284 - acc: 0.9913 - val_loss: 1.5169 - val_acc: 0.8171\n",
      "Epoch 20/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0358 - acc: 0.9887 - val_loss: 1.5244 - val_acc: 0.8217\n",
      "Epoch 21/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0351 - acc: 0.9894 - val_loss: 1.2648 - val_acc: 0.8306\n",
      "Epoch 22/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0265 - acc: 0.9922 - val_loss: 1.3534 - val_acc: 0.8352\n",
      "Epoch 23/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.0153 - acc: 0.9956 - val_loss: 1.4405 - val_acc: 0.8386\n",
      "Epoch 24/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0122 - acc: 0.9965 - val_loss: 1.4013 - val_acc: 0.8378\n",
      "Epoch 25/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0081 - acc: 0.9976 - val_loss: 1.0818 - val_acc: 0.8645\n",
      "Epoch 26/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.0106 - acc: 0.9958 - val_loss: 1.5909 - val_acc: 0.8312\n",
      "Epoch 27/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.0103 - acc: 0.9975 - val_loss: 1.2390 - val_acc: 0.8520\n",
      "Epoch 28/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0270 - acc: 0.9918 - val_loss: 1.0134 - val_acc: 0.8607\n",
      "Epoch 29/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0169 - acc: 0.9953 - val_loss: 1.2743 - val_acc: 0.8504\n",
      "Epoch 30/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0238 - acc: 0.9934 - val_loss: 1.5302 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "Epoch 31/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.0102 - acc: 0.9965 - val_loss: 1.3077 - val_acc: 0.8465\n",
      "Epoch 32/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0101 - acc: 0.9971 - val_loss: 1.6675 - val_acc: 0.8259\n",
      "Epoch 33/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0101 - acc: 0.9971 - val_loss: 1.2033 - val_acc: 0.8541\n",
      "Epoch 34/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0082 - acc: 0.9980 - val_loss: 0.9783 - val_acc: 0.8683\n",
      "Epoch 35/120\n",
      "5492/5492 [==============================] - 26s 5ms/step - loss: 0.0101 - acc: 0.9980 - val_loss: 1.3947 - val_acc: 0.8506\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0004050000192364678.\n",
      "Epoch 36/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0105 - acc: 0.9973 - val_loss: 1.1608 - val_acc: 0.8521\n",
      "Epoch 37/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0096 - acc: 0.9975 - val_loss: 1.1753 - val_acc: 0.8480\n",
      "Epoch 38/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 1.4573 - val_acc: 0.8447\n",
      "Epoch 39/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0057 - acc: 0.9982 - val_loss: 1.5774 - val_acc: 0.8428\n",
      "Epoch 40/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0053 - acc: 0.9987 - val_loss: 1.5330 - val_acc: 0.8435\n",
      "Epoch 41/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0048 - acc: 0.9985 - val_loss: 1.3644 - val_acc: 0.8556\n",
      "Epoch 42/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 1.4832 - val_acc: 0.8430\n",
      "Epoch 43/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0047 - acc: 0.9991 - val_loss: 1.3526 - val_acc: 0.8606\n",
      "Epoch 44/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 1.4696 - val_acc: 0.8467\n",
      "Epoch 45/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0075 - acc: 0.9982 - val_loss: 1.3938 - val_acc: 0.8486\n",
      "Epoch 46/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0136 - acc: 0.9958 - val_loss: 1.1964 - val_acc: 0.8672\n",
      "Epoch 47/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 1.4770 - val_acc: 0.8499\n",
      "Epoch 48/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0044 - acc: 0.9984 - val_loss: 1.4171 - val_acc: 0.8504\n",
      "Epoch 49/120\n",
      "5492/5492 [==============================] - 27s 5ms/step - loss: 0.0070 - acc: 0.9985 - val_loss: 1.4060 - val_acc: 0.8464\n",
      "Epoch 00049: early stopping\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load(\"s.npy\")\n",
    "y_train = np.load(\"s_l.npy\")\n",
    "X_test = np.load(\"s_a.npy\")\n",
    "y_test = np.load(\"s_a_l.npy\")\n",
    "t = X_train.shape\n",
    "X_train = X_train.reshape(t[0], t[1], t[2], 1)\n",
    "t = X_test.shape\n",
    "X_test = X_test.reshape(t[0], t[1], t[2], 1)\n",
    "for j in range(30):\n",
    "  gc.collect()\n",
    "time.sleep(5)\n",
    "for j in range(30):\n",
    "  gc.collect()\n",
    "mean = np.mean(X_train, axis = 0)\n",
    "std = np.std(X_train, axis = 0)\n",
    "X_train = stats.zscore(X_train)\n",
    "X_test = (X_test - mean)/std\n",
    "train_case_4(X_train, y_train, X_test, y_test, filename = \"spectrogram_case_4.txt\", epochs = 120)\n",
    "for j in range(30):\n",
    "  gc.collect()\n",
    "time.sleep(5)\n",
    "for j in range(30):\n",
    "  gc.collect()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "spectrogram_case 3_4_ptit.nickdangki.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
