{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "DkKvg4aK2waa",
    "outputId": "3693ccd3-2f4e-4fe3-a2ff-234cc2d1a466"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tải dữ liệu tử google drive lên Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmiCMXiT24X8"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/My\\ Drive/DuLieu/filter_feature/s_a_l.npy .\n",
    "!cp /content/drive/My\\ Drive/DuLieu/filter_feature/s_a.npy ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "colab_type": "code",
    "id": "bdFhJdSk3O78",
    "outputId": "972ede79-4081-42d0-a4e4-cea4f2361970"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from shutil import copyfile\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "kf = KFold(n_splits=5, shuffle= True, random_state=28)\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LE0bnZz3T6v"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(64, (6, 6), activation = \"relu\", padding = \"valid\", input_shape = (513, 94, 1)))\n",
    "  model.add(MaxPooling2D(2, 2))\n",
    "  model.add(Conv2D(64, (6, 6), activation = \"relu\", padding = \"valid\"))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(400))\n",
    "  model.add(Activation(\"relu\"))\n",
    "  model.add(Dense(150))\n",
    "  model.add(Activation(\"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(1, activation = \"sigmoid\"))\n",
    "  model.compile(optimizer= keras.optimizers.Adam(lr=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIlzcCiX3Xel"
   },
   "outputs": [],
   "source": [
    "class LogConfusionMatrix(keras.callbacks.Callback):\n",
    "  def __init__(self, names):\n",
    "    self.names = names\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.bestacc = 0.0\n",
    "    self.bestcf = None\n",
    "  def on_epoch_end(self, epoch, logs = {}):\n",
    "    if logs[\"val_acc\"] > self.bestacc:\n",
    "      self.bestacc = logs[\"val_acc\"] \n",
    "      y_pred = self.model.predict(self.validation_data[0])\n",
    "      y_pred = [1 * (x[0]>=0.5) for x in y_pred]\n",
    "      self.bestcf = confusion_matrix(self.validation_data[1], y_pred)\n",
    "  def on_train_end(self, logs = {}):\n",
    "    f = open(self.names, \"a\")\n",
    "    f.write(\"Best val_acc: \" + str(self.bestacc))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Best cf: \\n\" + str(self.bestcf))\n",
    "    f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCmwIapr3alt"
   },
   "outputs": [],
   "source": [
    "def train_case_2(X_aug, y_aug, filename, epochs = 40):\n",
    "  i = 1\n",
    "  for train_index, test_index in kf.split(X_aug):\n",
    "    print(\"Cross Valid: \" + str(i))\n",
    "    f = open(filename, \"a\")\n",
    "    f.write(\"Cross Valid: \" + str(i))\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    logcf = LogConfusionMatrix(filename)\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0.001, patience=18, verbose=1, mode='auto')\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = \"loss\",patience = 6, factor = 0.9, min_lr = 0.0001, verbose = 1)\n",
    "    model = create_model()\n",
    "    mean = np.mean(X_aug[train_index], axis = 0)\n",
    "    std = np.std(X_aug[train_index], axis = 0)\n",
    "    X_train = (X_aug[train_index] - mean) / std\n",
    "    y_train = y_aug[train_index]\n",
    "    X_test = (X_aug[test_index] - mean) / std\n",
    "    y_test = y_aug[test_index]\n",
    "    model.fit(X_train, y_train, epochs = epochs, batch_size = 64, verbose = 1, validation_data = (X_test, y_test), callbacks = [es, logcf, reduce_lr])\n",
    "    #print(confusion_matrix(y_test, y_pred))\n",
    "    del model\n",
    "    del X_train\n",
    "    del y_train\n",
    "    del X_test\n",
    "    del y_test\n",
    "    del mean\n",
    "    del std\n",
    "    for j in range(30):\n",
    "      gc.collect()\n",
    "    time.sleep(5)\n",
    "    for j in range(30):\n",
    "      gc.collect()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_B8HMZT2653"
   },
   "outputs": [],
   "source": [
    "X = np.load(\"s_a.npy\")\n",
    "y = np.load(\"s_a_l.npy\")\n",
    "t = X.shape\n",
    "X = X.reshape(t[0], t[1], t[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train và ghi kết quả vào file spectrogram_case_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9sp74MhI3iTN",
    "outputId": "6274f9f5-b36a-45a6-b290-f3e5102635d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Valid: 1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 30968 samples, validate on 7742 samples\n",
      "Epoch 1/120\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "30968/30968 [==============================] - 69s 2ms/step - loss: 0.3204 - acc: 0.8660 - val_loss: 0.1829 - val_acc: 0.9251\n",
      "Epoch 2/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.1910 - acc: 0.9228 - val_loss: 0.1436 - val_acc: 0.9393\n",
      "Epoch 3/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.1506 - acc: 0.9384 - val_loss: 0.1217 - val_acc: 0.9516\n",
      "Epoch 4/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.1221 - acc: 0.9534 - val_loss: 0.1000 - val_acc: 0.9653\n",
      "Epoch 5/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.1020 - acc: 0.9613 - val_loss: 0.0966 - val_acc: 0.9646\n",
      "Epoch 6/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0804 - acc: 0.9695 - val_loss: 0.0749 - val_acc: 0.9718\n",
      "Epoch 7/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0690 - acc: 0.9740 - val_loss: 0.0710 - val_acc: 0.9746\n",
      "Epoch 8/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0607 - acc: 0.9775 - val_loss: 0.0655 - val_acc: 0.9765\n",
      "Epoch 9/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0500 - acc: 0.9804 - val_loss: 0.0752 - val_acc: 0.9737\n",
      "Epoch 10/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0430 - acc: 0.9842 - val_loss: 0.0570 - val_acc: 0.9815\n",
      "Epoch 11/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0388 - acc: 0.9855 - val_loss: 0.0608 - val_acc: 0.9802\n",
      "Epoch 12/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0349 - acc: 0.9880 - val_loss: 0.0673 - val_acc: 0.9774\n",
      "Epoch 13/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0292 - acc: 0.9895 - val_loss: 0.0687 - val_acc: 0.9779\n",
      "Epoch 14/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0234 - acc: 0.9922 - val_loss: 0.0745 - val_acc: 0.9788\n",
      "Epoch 15/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0221 - acc: 0.9924 - val_loss: 0.0680 - val_acc: 0.9800\n",
      "Epoch 16/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0225 - acc: 0.9923 - val_loss: 0.0635 - val_acc: 0.9797\n",
      "Epoch 17/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0828 - val_acc: 0.9771\n",
      "Epoch 18/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.0651 - val_acc: 0.9808\n",
      "Epoch 19/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0156 - acc: 0.9951 - val_loss: 0.0681 - val_acc: 0.9788\n",
      "Epoch 20/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0883 - val_acc: 0.9789\n",
      "Epoch 21/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0184 - acc: 0.9938 - val_loss: 0.0777 - val_acc: 0.9817\n",
      "Epoch 22/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.0796 - val_acc: 0.9808\n",
      "Epoch 23/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0151 - acc: 0.9953 - val_loss: 0.0686 - val_acc: 0.9811\n",
      "Epoch 24/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0135 - acc: 0.9958 - val_loss: 0.0894 - val_acc: 0.9820\n",
      "Epoch 25/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0712 - val_acc: 0.9820\n",
      "Epoch 26/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0107 - acc: 0.9967 - val_loss: 0.0694 - val_acc: 0.9830\n",
      "Epoch 27/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0098 - acc: 0.9970 - val_loss: 0.0920 - val_acc: 0.9813\n",
      "Epoch 28/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0801 - val_acc: 0.9827\n",
      "Epoch 29/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0111 - acc: 0.9968 - val_loss: 0.0811 - val_acc: 0.9823\n",
      "Epoch 30/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0191 - acc: 0.9946 - val_loss: 0.0617 - val_acc: 0.9848\n",
      "Epoch 31/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0753 - val_acc: 0.9846\n",
      "Epoch 32/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.1049 - val_acc: 0.9839\n",
      "Epoch 33/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0863 - val_acc: 0.9815\n",
      "Epoch 34/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0090 - acc: 0.9971 - val_loss: 0.0975 - val_acc: 0.9819\n",
      "Epoch 35/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0705 - val_acc: 0.9828\n",
      "Epoch 36/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0203 - acc: 0.9944 - val_loss: 0.0708 - val_acc: 0.9839\n",
      "Epoch 37/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0061 - acc: 0.9983 - val_loss: 0.0705 - val_acc: 0.9851\n",
      "Epoch 38/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0881 - val_acc: 0.9836\n",
      "Epoch 39/120\n",
      "30968/30968 [==============================] - 57s 2ms/step - loss: 0.0054 - acc: 0.9986 - val_loss: 0.0653 - val_acc: 0.9844\n",
      "Epoch 40/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0062 - acc: 0.9981 - val_loss: 0.0642 - val_acc: 0.9854\n",
      "Epoch 41/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.1199 - val_acc: 0.9814\n",
      "Epoch 42/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0056 - acc: 0.9982 - val_loss: 0.0962 - val_acc: 0.9840\n",
      "Epoch 43/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0054 - acc: 0.9979 - val_loss: 0.0728 - val_acc: 0.9840\n",
      "Epoch 44/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0078 - acc: 0.9979 - val_loss: 0.0948 - val_acc: 0.9817\n",
      "Epoch 45/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0148 - acc: 0.9960 - val_loss: 0.0735 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "Epoch 46/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0837 - val_acc: 0.9826\n",
      "Epoch 47/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0821 - val_acc: 0.9857\n",
      "Epoch 48/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0705 - val_acc: 0.9831\n",
      "Epoch 49/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.0890 - val_acc: 0.9845\n",
      "Epoch 50/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.0915 - val_acc: 0.9832\n",
      "Epoch 51/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0844 - val_acc: 0.9845\n",
      "Epoch 52/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0943 - val_acc: 0.9835\n",
      "Epoch 53/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.1092 - val_acc: 0.9822\n",
      "Epoch 54/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0906 - val_acc: 0.9828\n",
      "Epoch 55/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0069 - acc: 0.9983 - val_loss: 0.0953 - val_acc: 0.9831\n",
      "Epoch 56/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0793 - val_acc: 0.9830\n",
      "Epoch 57/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.1152 - val_acc: 0.9818\n",
      "Epoch 58/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0886 - val_acc: 0.9845\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0004050000192364678.\n",
      "Epoch 59/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0921 - val_acc: 0.9819\n",
      "Epoch 60/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0808 - val_acc: 0.9851\n",
      "Epoch 61/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0930 - val_acc: 0.9854\n",
      "Epoch 62/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0125 - acc: 0.9977 - val_loss: 0.1048 - val_acc: 0.9808\n",
      "Epoch 63/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0795 - val_acc: 0.9853\n",
      "Epoch 00063: early stopping\n",
      "Cross Valid: 2\n",
      "Train on 30968 samples, validate on 7742 samples\n",
      "Epoch 1/120\n",
      "30968/30968 [==============================] - 58s 2ms/step - loss: 0.2970 - acc: 0.8734 - val_loss: 0.1722 - val_acc: 0.9296\n",
      "Epoch 2/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.1620 - acc: 0.9364 - val_loss: 0.1252 - val_acc: 0.9516\n",
      "Epoch 3/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.1280 - acc: 0.9507 - val_loss: 0.0977 - val_acc: 0.9632\n",
      "Epoch 4/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0975 - acc: 0.9622 - val_loss: 0.0797 - val_acc: 0.9694\n",
      "Epoch 5/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0833 - acc: 0.9678 - val_loss: 0.0740 - val_acc: 0.9724\n",
      "Epoch 6/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0720 - acc: 0.9726 - val_loss: 0.0871 - val_acc: 0.9682\n",
      "Epoch 7/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0635 - acc: 0.9761 - val_loss: 0.0686 - val_acc: 0.9749\n",
      "Epoch 8/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0550 - acc: 0.9800 - val_loss: 0.0781 - val_acc: 0.9726\n",
      "Epoch 9/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0466 - acc: 0.9825 - val_loss: 0.0648 - val_acc: 0.9791\n",
      "Epoch 10/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0410 - acc: 0.9848 - val_loss: 0.0881 - val_acc: 0.9749\n",
      "Epoch 11/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0359 - acc: 0.9869 - val_loss: 0.0665 - val_acc: 0.9796\n",
      "Epoch 12/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0304 - acc: 0.9898 - val_loss: 0.0603 - val_acc: 0.9788\n",
      "Epoch 13/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0304 - acc: 0.9889 - val_loss: 0.0643 - val_acc: 0.9802\n",
      "Epoch 14/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0248 - acc: 0.9912 - val_loss: 0.0645 - val_acc: 0.9799\n",
      "Epoch 15/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0210 - acc: 0.9921 - val_loss: 0.0664 - val_acc: 0.9778\n",
      "Epoch 16/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0313 - acc: 0.9892 - val_loss: 0.0692 - val_acc: 0.9801\n",
      "Epoch 17/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0223 - acc: 0.9925 - val_loss: 0.0704 - val_acc: 0.9804\n",
      "Epoch 18/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0186 - acc: 0.9937 - val_loss: 0.0698 - val_acc: 0.9818\n",
      "Epoch 19/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0159 - acc: 0.9947 - val_loss: 0.0724 - val_acc: 0.9806\n",
      "Epoch 20/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0193 - acc: 0.9934 - val_loss: 0.0737 - val_acc: 0.9814\n",
      "Epoch 21/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0156 - acc: 0.9948 - val_loss: 0.0744 - val_acc: 0.9795\n",
      "Epoch 22/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0157 - acc: 0.9947 - val_loss: 0.0816 - val_acc: 0.9797\n",
      "Epoch 23/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0162 - acc: 0.9945 - val_loss: 0.0785 - val_acc: 0.9784\n",
      "Epoch 24/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0764 - val_acc: 0.9796\n",
      "Epoch 25/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.0846 - val_acc: 0.9805\n",
      "Epoch 26/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0206 - acc: 0.9932 - val_loss: 0.0722 - val_acc: 0.9805\n",
      "Epoch 27/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0108 - acc: 0.9963 - val_loss: 0.0801 - val_acc: 0.9810\n",
      "Epoch 28/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0147 - acc: 0.9952 - val_loss: 0.0815 - val_acc: 0.9800\n",
      "Epoch 29/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0129 - acc: 0.9961 - val_loss: 0.0688 - val_acc: 0.9792\n",
      "Epoch 30/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0716 - val_acc: 0.9819\n",
      "Epoch 31/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0088 - acc: 0.9971 - val_loss: 0.0893 - val_acc: 0.9809\n",
      "Epoch 32/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0079 - acc: 0.9974 - val_loss: 0.0707 - val_acc: 0.9814\n",
      "Epoch 33/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0102 - acc: 0.9966 - val_loss: 0.0812 - val_acc: 0.9808\n",
      "Epoch 34/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0108 - acc: 0.9969 - val_loss: 0.0781 - val_acc: 0.9801\n",
      "Epoch 35/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0108 - acc: 0.9968 - val_loss: 0.0761 - val_acc: 0.9832\n",
      "Epoch 36/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0147 - acc: 0.9955 - val_loss: 0.0867 - val_acc: 0.9818\n",
      "Epoch 37/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0089 - acc: 0.9969 - val_loss: 0.1078 - val_acc: 0.9795\n",
      "Epoch 38/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0900 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0004500000213738531.\n",
      "Epoch 39/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0080 - acc: 0.9978 - val_loss: 0.0914 - val_acc: 0.9799\n",
      "Epoch 40/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.0778 - val_acc: 0.9823\n",
      "Epoch 41/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.1154 - val_acc: 0.9805\n",
      "Epoch 42/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.0755 - val_acc: 0.9824\n",
      "Epoch 43/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.0782 - val_acc: 0.9832\n",
      "Epoch 44/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0084 - acc: 0.9980 - val_loss: 0.0959 - val_acc: 0.9809\n",
      "Epoch 45/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0841 - val_acc: 0.9826\n",
      "Epoch 46/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0726 - val_acc: 0.9831\n",
      "Epoch 47/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.0829 - val_acc: 0.9833\n",
      "Epoch 48/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0771 - val_acc: 0.9836\n",
      "Epoch 49/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0868 - val_acc: 0.9845\n",
      "Epoch 50/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0819 - val_acc: 0.9836\n",
      "Epoch 51/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0066 - acc: 0.9983 - val_loss: 0.1026 - val_acc: 0.9820\n",
      "Epoch 52/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0048 - acc: 0.9982 - val_loss: 0.0791 - val_acc: 0.9828\n",
      "Epoch 53/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0059 - acc: 0.9984 - val_loss: 0.1218 - val_acc: 0.9789\n",
      "Epoch 54/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0039 - acc: 0.9985 - val_loss: 0.1067 - val_acc: 0.9826\n",
      "Epoch 55/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0076 - acc: 0.9977 - val_loss: 0.1077 - val_acc: 0.9789\n",
      "Epoch 56/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0061 - acc: 0.9983 - val_loss: 0.0919 - val_acc: 0.9833\n",
      "Epoch 57/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0939 - val_acc: 0.9826\n",
      "Epoch 58/120\n",
      "30968/30968 [==============================] - 56s 2ms/step - loss: 0.0032 - acc: 0.9992 - val_loss: 0.0961 - val_acc: 0.9831\n",
      "Epoch 59/120\n",
      "13760/30968 [============>.................] - ETA: 29s - loss: 0.0128 - acc: 0.9961Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "train_case_2(X, y, \"spectrogram_case_2.txt\", 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qg6wToUh_FeH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Untitled26.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
